
import chart_studio
import plotly.express as px
import os
import pandas as pd
from googleapiclient.discovery import build
import re
import joblib
import numpy as np
import neattext.functions as nfx



def generate_sunburst_chart(youtube_url):

    # -*- coding: utf-8 -*-
    """Comment_sunburst.ipynb

    Automatically generated by Colab.

    Original file is located at
        https://colab.research.google.com/drive/1DgTBVgeJh3voOi6UoH8QWwmMqvfnlOq3

    ## Frontend to backend Data conversion
    """

    

    # Set up YouTube API
    youtube = build("youtube", "v3", developerKey="AIzaSyA_jV9_i2HJF_kzFqVLNeiSn2svb4I5tuc")



    def extract_video_id(youtube_url):
        """
        Extracts the video ID from a YouTube video URL.
        """
        # Regular expression pattern to match YouTube video IDs
        pattern = r"(?:https?:\/\/)?(?:www\.)?(?:youtube\.com\/(?:[^\/\n\s]+\/\S+\/|(?:v|e(?:mbed)?)\/|\S*?[?&]v=)|youtu\.be\/)([a-zA-Z0-9_-]{11})"

        # Attempt to match the pattern in the URL
        match = re.search(pattern, youtube_url)

        # If a match is found, return the video ID
        if match:
            return match.group(1)
        else:
            return None

    # Example usage
    
    video_id = extract_video_id(youtube_url)


    # Initialize comments list
    comments = []


    try:
        # Fetch comments from YouTube video
        request = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            maxResults=1000,  # Set to a large value to fetch more comments per request
        )

        # Fetch all comments
        while request:
            response = request.execute()
            # video_title = response["items"][0]["snippet"]["title"]
            for item in response["items"]:
                comments.append(item["snippet"]["topLevelComment"]["snippet"]["textDisplay"])
                # print(video_title)

            request = youtube.commentThreads().list_next(request, response)

    except Exception as e:
        print("An error occurred:", e)

    
    # Print the comments
    # for i, comment in enumerate(comments, start=1):
    #     print(f"Comment {i}: {comment}")
    video_id = extract_video_id(youtube_url)

    # Fetch video details to get the title
    video_request = youtube.videos().list(
        part="snippet",
        id=video_id
    )
    video_response = video_request.execute()

    # Extract video title
    video_title = video_response["items"][0]["snippet"]["title"]
    
    

    pipe_lr_loaded = joblib.load('logistic_regression_model.pkl')

    # Load the saved Random Forest model
    pipe_svm_loaded = joblib.load('random_forest_model.pkl')

    # Preprocess the comments
    preprocessed_comments = [nfx.remove_userhandles(comment) for comment in comments]
    preprocessed_comments = [nfx.remove_stopwords(comment) for comment in preprocessed_comments]

    # Use the loaded models to make predictions on the preprocessed comments
    y_pred_lr_loaded = pipe_lr_loaded.predict(preprocessed_comments)
    y_pred_svm_loaded = pipe_svm_loaded.predict(preprocessed_comments)

    # Perform further analysis as needed (e.g., calculate accuracy)
    # For example, if you have ground truth labels (y_test), you can calculate accuracy
    # accuracy_lr_loaded = accuracy_score(y_test, y_pred_lr_loaded)
    # accuracy_svm_loaded = accuracy_score(y_test, y_pred_svm_loaded)

    """## Backend to Frontend Data conversion"""

    

    # Assuming your array is named 'emotion_array'
    # Replace this with your actual array
    emotion_array = np.array(y_pred_svm_loaded)  # Replace [...] with your array data

    # Count occurrences of each emotion
    unique_emotions, counts = np.unique(emotion_array, return_counts=True)

    # Create a dictionary to store the counts
    emotion_counts = dict(zip(unique_emotions, counts))

    # Create lists for the two columns
    tertiary_emotions = list(emotion_counts.keys())
    values = list(emotion_counts.values())

    # Create a new array with two columns and add column names
    new_array = np.array([tertiary_emotions, values]).T
    column_names = ["Tertiary emotion", "value"]
    new_array_with_names = np.vstack([column_names, new_array])

    # Write the new array to a CSV file
    np.savetxt("new_file.csv", new_array_with_names, delimiter=",", fmt="%s")

    """## Preprocessing for Sunburst chart"""

    

    # Read the second CSV file into a DataFrame
    second_csv_file = "new_file.csv"
    second_df = pd.read_csv(second_csv_file)


    # Read the first CSV file into a DataFrame
    first_csv_file = "sunburst_input.csv"
    first_df = pd.read_csv(first_csv_file)




    # Iterate over the rows of the second DataFrame
    for index, row in second_df.iterrows():
        # Find the corresponding row in the first DataFrame based on the tertiary emotion
        first_row_index = first_df.index[first_df['Tertiary emotion'] == row['Tertiary emotion']]

        # Copy the values from the second DataFrame to the corresponding rows in the first DataFrame
        for idx in first_row_index:
            first_df.at[idx, 'value'] = row['value']

    # Save the modified DataFrame to a new CSV file
    output_csv_file = "output_file.csv"
    first_df.to_csv(output_csv_file, index=False)

    """## Printing the sunburst chart"""

   
    username = 'pranay777'
    api_key = 'XT6nm1Hb4xFrcucT8k0g'

    # Set Chart Studio credentials
    chart_studio.tools.set_credentials_file(username=username, api_key=api_key)

    df = pd.read_csv("output_file.csv")
    df['Emotion'] = " "

    fig = px.sunburst(
        data_frame=df,
        path=["Emotion", "Primary emotion", "Secondary emotion", "Tertiary emotion"],
        values='value',  # Use the 'value' column for sizing the sectors

    )

    # Show the chart
    fig.show()
   
    file_name = video_title + '.html'
    file_path = os.path.join( 'final_output' , file_name)
    fig.write_html(file_path)
    return video_title , file_path
